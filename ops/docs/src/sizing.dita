<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="sizing">
<title>Sizing considerations</title>
<body>
<p>A node is a machine in the cluster (virtual or physical) with Docker Engine running on
it. There are two types of nodes: managers and workers. UCP will run on the manager nodes. 
Although DTR runs on a worker node, Docker does not recommend running other 
application containers on them. </p>
<p>To decide what size the node should be in terms of CPU, RAM, and storage resources,
consider the following:</p>
<ol>
<li>All nodes should at least fulfil the minimal requirements, for UCP 3.0, 8GB of RAM
and 3GB of storage. For production systems, 16GB of RAM is recommended for manager nodes. 
More detailed requirements are in the Docker EE UCP documentation
 at <xref href="https://docs.docker.com/ee/ucp/admin/install/system-requirements/" format="html" scope="external"/>. </li>
<li>UCP Controller nodes should be provided with more than the minimal requirements if other workloads run on them. </li>
<li>Ideally, worker node size will vary based on your workloads so it is impossible to
define a universal standard size. </li>
<li>Other considerations like target density (average number of containers per node),
whether one standard node type or several are preferred, and other operational
considerations might also influence sizing. </li>
</ol>

<p>If possible, node size should be determined by experimentation and testing actual
workloads; and they should be refined iteratively. A good starting point is to select a
standard or default machine type in your environment and use this size only. If your
standard machine type provides more resources than the UCP Controller nodes need, it makes
sense to have a smaller node size for these. Whatever the starting choice, it is
important to monitor resource usage and cost to improve the model.</p>


<p>For this solution, the following tables describe
sizing configurations, assuming 3 Linux workers and 3 Windows workers. The vCPU allocations are described in <xref href="sizing.dita#sizing/vcpu-table-conref"/> while the memory
allocation is described in <xref href="sizing.dita#sizing/memory-alloc-table-conref"/>.</p>

<table id="vcpu-table-conref" conref="vcpu-table.dita#vcpu-table/vcpu-table-content">
<title>vCPU</title>
<tgroup cols="4">
<tbody>
<row>
<entry/>
</row>
</tbody>
</tgroup>

</table>


<note type="note">
<p>In the case of one ESX host failure, 2 nodes are enough to accommodate the amount
of vCPU required</p>
</note>

<p/>
<!--<p>Table 2. Memory allocation</p>-->
<table id="memory-alloc-table-conref" conref="memory-alloc-table.dita#memory-alloc-table/memory-alloc-table-content">
<title>Memory allocation</title>
<tgroup cols="4">
<tbody>
<row>
<entry/>
</row>
</tbody>
</tgroup>

</table>


<note type="note">
<p>In the case of one ESX host failure, the two surviving hosts can accommodate the
amount of RAM required for all VMs.</p>
</note>



<p>For a 6 node solution,  <xref href="sizing.dita#sizing/memory-alloc-table-6node-conref"/> outlines the 
 memory requirements where the control plane is on 3 nodes and the worker nodes are on the other 3 nodes.
In this example, it is assumed that there are 2 Linux worker nodes and 1 Windows worker node, but the actual number of
worker nodes is not limited to 3 and depends entirely on the workload requirements.
</p>


<table id="memory-alloc-table-6node-conref" conref="memory-alloc-table-6node.dita#memory-alloc-table-6node/memory-alloc-table-6node-content">
<title>Sample memory allocation for 6 nodes</title>
<tgroup cols="7">
<tbody>
<row>
<entry/>
</row>
</tbody>
</tgroup>

</table>


<note type="note">
<p>In the case of one ESX host failure, the surviving hosts can accommodate the
amount of RAM required for all VMs.</p>
</note>
 
</body>
</topic>
