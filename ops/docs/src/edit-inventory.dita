<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="edit-inventory">
<title>Editing the inventory</title>
<body>
<p>The inventory is the file named <codeph>vm_hosts</codeph> in the
<codeph>~/<ph conkeyref="conrefs/gitrepo-string"/></codeph> directory. You need to edit this file to
describe the configuration you want to deploy.</p>

<p>The nodes inside the inventory are organized in groups. The groups are defined by
brackets and the group names are static so they must not be changed. Other fields
(hostnames, specifications, IP addressesâ€¦) are edited to match your setup.
The groups are as follows:</p>
<ul>
<li><codeph>[ucp_main]</codeph>: A group containing one single node which will be the
main UCP node and swarm leader. Do not add more than one node under this group.</li>
<li><codeph>[ucp]</codeph>: A group containing all the UCP nodes, including the main UCP
node. Typically you should have either 3 or 5 nodes under this group.</li>
<li><codeph>[dtr_main]</codeph>: A group containing one single node which will be the
first DTR node to be installed. Do not add more than one node under this group.</li>
<li><codeph>[dtr]</codeph>: A group containing all the DTR nodes, including the main DTR
node. Typically you should have either 3 or 5 nodes under this group.</li>
    
<li><codeph>[worker]</codeph>: A group containing all the Linux worker nodes.</li>
<li><codeph>[win_worker]</codeph>: A group containing all the Windows worker nodes.</li>
    

<li><codeph>[nfs]</codeph>: A group containing one single node which will be the NFS
node. Do not add more than one node under this group.</li>
<li><codeph>[logger]</codeph>: A group containing one single node which will be the
logger node. Do not add more than one node under this group.</li>
<li><codeph>[local]</codeph>: A group containing the local Ansible host. It contains an
entry that should not be modified.</li>
</ul>

<p>If you are deploying the new loadbalancers, using floating IPs managed by <codeph>keepalived</codeph>:</p>
    
<ul>
<li><codeph>[lb1]</codeph>: A group containing one single node which will be the load
balancer for the UCP nodes. Do not add more than one node under this group.</li>    
<li><codeph>[lb2]</codeph>: A group containing one single node which will be the load
balancer for the DTR nodes. Do not add more than one node under this group.</li>   
<li><codeph>loadbalancer</codeph>: A group containing the UCP, DTR and any worker load balancers you are deploying.</li>    
</ul>
    
    
<p>If you are using the legacy, standalone load balancers:</p>
<ul>
<li><codeph>[ucp_lb]</codeph>: A group containing one single node which will be the load
balancer for the UCP nodes. Do not add more than one node under this group.</li>
<li><codeph>[dtr_lb]</codeph>: A group containing one single node which will be the load
balancer for the DTR nodes. Do not add more than one node under this group.</li>
<li><codeph>[worker_lb]</codeph>: A group containing one single node which will be the
load balancer for the worker nodes. Do not add more than one node under this
group.</li>    
<li><codeph>[lbs]</codeph>: A group containing all the legacy standalone load balancers. This group will
have 3 nodes, also defined individually in the three groups above.</li>    
</ul>    
    

<p>There are also a few special groups:</p>
<ul>
<li>[docker:children]: A group of groups including all the nodes where Docker will be
installed.</li>
<li>[vms:children]: A group of groups including all the Virtual Machines involved, with the exception of the local host.</li>
</ul>

<p>Finally, you will find some variables defined for each group:</p>
<ul>
<li>[vms:vars]: A set of variables defined for all VMs. Currently only the size of the
boot disk is defined here.</li>
<li>[ucp:vars]: A set of variables defined for all nodes in the [<codeph>ucp</codeph>]
group.</li>
<li>[dtr:vars]: A set of variables defined for all nodes in the [<codeph>dtr</codeph>]
group.</li>
<li>[worker:vars]: A set of variables defined for all nodes in the
[<codeph>worker</codeph>] group.</li>
<li>[win_worker:vars]: A set of variables defined for all nodes in the
[<codeph>win_worker</codeph>] group.</li>
<li>[loadbalancer:vars]: A set of variables defined for all nodes in the [<codeph>loadbalancer</codeph>]
group.</li>    
<li>[lbs:vars]: A set of variables defined for all nodes in the [<codeph>lbs</codeph>]
group.</li>
<li>[nfs:vars]: A set of variables defined for all nodes in the [<codeph>nfs</codeph>]
group.</li>
<li>[logger:vars]: A set of variables defined for all nodes in the
[<codeph>logger</codeph>] group.</li>
</ul>

<p>If you wish to configure your nodes with different specifications to the ones
defined by the group, it is possible to declare the same variables at the node level,
overriding the group value. For instance, you could have one of your workers with higher
specifications by setting:</p>

<codeblock>[worker] 
worker01 ip_addr='10.0.0.10/16' esxi_host='esxi1.domain.local' 
worker02 ip_addr='10.0.0.11/16' esxi_host='esxi1.domain.local' 
worker03 ip_addr='10.0.0.12/16' esxi_host='esxi1.domain.local' cpus='16' ram'32768' 

[worker:vars] 
cpus='4' ram='16384' disk2_size='200'</codeblock>

<p>In the example above, the <codeph>worker03</codeph> node would have 4 times more CPU and
double the RAM compared to the rest of the worker nodes.</p>
<p>The different variables you can use are described in 
<xref href="edit-inventory.dita#edit-inventory/variables-local-table-content"></xref> below. They are all
mandatory unless otherwise specified.</p>


<table id="variables-local-table-content" conref="variables-table.dita#variables-table/variables-table-content">
<title>Variables</title>
<tgroup cols="3">
<tbody>
<row>
<entry/>
</row>
</tbody>
</tgroup>
</table>


</body>
</topic>
