<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="playbooks-overview">
<title>Overview of the playbooks</title>
<body/>

<topic id="site">
<title>site.yml and related playbooks</title>
<body>  

<p>The Ansible playbooks are available to download at 
    <xref href="https://github.com/HewlettPackard/Docker-Synergy" format="html" scope="external"/>. 
    Once you have cloned the repository, change directory to <codeph>/root/Docker-Synergy/ops</codeph>.</p>

<p>There are two main entry points for the playbooks:</p>
    
<ul>
<li><codeph>site.yml</codeph> deploys a Linux-only environment</li>
<li><codeph>hybrid.yml</codeph> deploys a mixed environment with Linux and Windows worker nodes</li>    
</ul>
 
<p>The playbook <codeph>./site.yml</codeph> is the day 0 playbook you use to deploy the solution and it invokes the 
following playbooks:</p>
    
<ul>
	
<li><codeph>playbooks/create_vms.yml</codeph> will create all the necessary virtual machines for 
the environment from the VM Template defined in the <codeph>vm_template</codeph> variable. 

All Linux VMs are now created in one go, regardless of the number of drives they have. 
This playbook also has the potential to configure additional network adapters.
</li>



<li><codeph>playbooks/config_networking.yml</codeph> will configure the network settings in all the virtual machines.</li>

<li><codeph>playbooks/resize_syspart.yml</codeph> resizes the logical volume that holds the <codeph>/</codeph> partition 
    of the Linux VMs to use all the space available on the drive. As of now, there is no equivalent for the Windows machines.</li>

<li><codeph>playbooks/config_subscription.yml</codeph> registers and subscribes all virtual machines to the Red Hat Customer Portal. 
This is only needed if you pull packages from Red Hat. This playbook is commented out by default but you should uncomment 
it to make sure each VM registers with the Red Hat portal. It is commented out so that you can test the deployment first 
without having to unregister all the VMs from the Red Hat Customer Portal between each test. If you are using an 
internal repository, as described in the section "Create a VM template", you can keep this playbook commented out.</li>



<li><codeph>playbooks/install_haproxy.yml</codeph> installs and configures the HAProxy package in the 
load balancer nodes. HAProxy is the tool chosen to implement load balancing for UCP nodes, DTR nodes and worker nodes.</li>


<li><codeph>playbooks/config_ntp.yml</codeph> configures the <b>chrony</b> client package in all virtual machines in 
order to have a synchronized clock across the environment. It will use the list of servers specified in the 
<codeph>ntp_servers</codeph> variable in the file <codeph>group_vars/vars</codeph>.</li>


<li><codeph>playbooks/install_docker.yml</codeph> installs Docker along with all of its dependencies.</li>


<li><codeph>playbooks/install_rsyslog.yml</codeph> installs and configures <b>rsyslog</b> in the logger node 
and in all Docker nodes. The logger node will be configured to receive all <codeph>syslogs</codeph> on port 514 and 
the Docker nodes will be configured to send all logs (including container logs) to the logger node.</li>

<li><codeph>playbooks/config_docker_lvs.yml</codeph> performs a set of operations on the Docker nodes 
in order to create a partition on the second disk and carry out the LVM configuration, 
required for a sound Docker installation.</li>


<li><codeph>playbooks/docker_post_config.yml</codeph> performs a variety of tasks to complete 
the installation of the Docker environment, including configuration of the HTTP/HTTPS proxies, if any, 
and installation of the VMware vSphere Storage for Docker volume plugin.</li>


<li><codeph>playbooks/install_nfs_server.yml</codeph> installs and configures an NFS server on the NFS node.

<p>This playbook has been updated to configure a third drive which is used to hold the data of the persistent volumes 
created with the NFS provisioner. This default size for this drive is purposefully kept small because 
using the NFS VM to store persistent volumes is not recommended for production use. However, this can be useful for demo purposes.</p>
</li>


<li><codeph>playbooks/install_nfs_clients.yml</codeph> installs the required packages on the DTR nodes 
to be able to mount an NFS share.</li>


<li><codeph>playbooks/create_main_ucp.yml</codeph> installs and
configures the first Docker UCP instance on the target node
defined by the group <codeph>ucp_main</codeph> in the
<codeph>vm_hosts</codeph> inventory.</li>


<li><codeph>playbooks/scale_ucp.yml</codeph> installs and configures
additional instances of UCP on the target node defined by
the group <codeph>ucp</codeph> in the
<codeph>vm_hosts</codeph> inventory, except for the node defined in the group
<codeph>ucp_main</codeph>.</li>

	

<li><codeph>playbooks/create_main_dtr.yml</codeph> installs and
configures the first Docker DTR instance on the target nodes
defined by the group <codeph>dtr_main</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>

	
<li><codeph>playbooks/scale_workers.yml</codeph> installs and configures additional Linux workers 
on the target nodes defined by the group <codeph>worker</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>

	

<li><codeph>playbooks/install_logspout.yml</codeph> installs and configures <b>Logspout</b> on all Docker nodes. 
Logspout is responsible for sending logs produced by containers running on the Docker nodes to the central logger VM.
By default, this playbook is commented out in <codeph>site.yml</codeph>.
</li>


<li><codeph>playbooks/config_monitoring.yml</codeph> configures a monitoring system for the Docker environment 
based on Grafana, Prometheus, cAdvisor and node-exporter Docker containers. By default, this playbook is commented out in <codeph>site.yml</codeph>, so if you want to use the solution to automatically
deploy a Prometheus/Grafana monitoring system, you must explicitly uncomment both this and the <codeph>playbooks/install_logspout.yml</codeph>
playbook.
</li>



<!--<li><codeph>playbooks/monitoring.yml</codeph> installs and configures the Splunk Universal Forwarders.
The variable <codeph>monitoring_stack</codeph> in <codeph>group_vars/vars</codeph> is used to specify the type of deployment you want.	
A value of <codeph>splunk_demo</codeph> will result in this playbook deploying a Splunk enterprise instance for demo purposes. A value of <codeph>splunk</codeph>
is used to configure an external production Splunk deployment.
</li>-->
    
<li><codeph>playbooks/splunk_demo.yml</codeph> installs a demo of Splunk Enterprise in the cluster (if the <codeph>splunk_demo</codeph> deployment option is selected. A value of <codeph>splunk</codeph>
is used to configure an external production Splunk deployment.)</li>    

<li><codeph>playbooks/splunk_uf.yml</codeph> installs and configures the Splunk Universal Forwarder on each Linux machine in the inventory</li>


<li><codeph>playbooks/config_scheduler.yml</codeph> configures the scheduler to prevent regular users 
(i.e. non-admin users) scheduling containers on the Docker nodes running instances of UCP and DTR.</li>
	

<li><codeph>playbooks/scale_dtr.yml</codeph> installs and configures additional instances (or replicas) 
of DTR on the target nodes defined by the group <codeph>dtr</codeph> in the <codeph>vm_hosts</codeph> inventory, 
with the exception of the node defined in the group <codeph>dtr_main</codeph>.</li>

	

<li><codeph>playbooks/reconfigure_dtr.yml</codeph>	is used to reconfigure DTR with the FQDN of the UCP Load Balancer and also 
enables image scanning.
</li>

<li><codeph>playbooks/install_sysdig.yml</codeph> is used to configure Sysdig. It opens the required port in the firewall, 
and installs the latest version of the Sysdig agent image on the nodes. By default, this playbook is commented out in <codeph>site.yml</codeph>, so if you want to use the solution to 
automatically configure Sysdig, you must uncomment this line.
</li>	
</ul>
    
</body> 
</topic>


<topic id="windows-playbooks">
<title>Windows playbooks</title>
    
<body>
<p><codeph>hybrid.yml</codeph> invokes the following  Windows-specific playbooks, together with the preceding ones for Linux.</p>  
    
<ul>  
     
<li><codeph>playbooks/create_windows_vms.yml</codeph> will create all the necessary Windows 2016 VMs 
for the environment based on the Windows VM Template defined in the <codeph>win_vm_template</codeph> variable.</li>   
    

<li><codeph>playbooks/install_docker_windows.yml</codeph> installs Docker along with all its dependencies 
on your Windows VMs</li>


<li><codeph>playbooks/scale_workers_win.yml</codeph> installs and configures additional Windows workers on the target nodes 
    defined by the group <codeph>win_worker</codeph> in the <codeph>vm_hosts</codeph> inventory.</li>
 
<li><codeph>playbooks/splunk_uf_win.yml</codeph> installs and configures the Splunk Universal Forwarder on each Windows machine in the inventory.</li>
 
</ul> 
</body>    
</topic>
    

<topic id="backup-restore-playbooks">
<title>Backup and restore playbooks</title>
    
<body>
    
<section>
<p>Best practices and procedures are described in the section <xref href="backup-restore.dita"/>. The following playbooks are used to perform backups:</p>
    
<ul>
<li><codeph>playbooks/backup_swarm.yml</codeph> is used to back up the swarm data</li>
<li><codeph>playbooks/backup_ucp.yml</codeph> is used to back up UCP</li>
<li><codeph>playbooks/backup_dtr_meta.yml</codeph> is used to back up DTR metadata</li>
<li><codeph>playbooks/backup_dtr_images.yml</codeph> is used to back up DTR images</li>
</ul>


</section>
    
<section>
<p>The following playbooks are used to restore the system:</p>
    
<ul>
<li><codeph>playbooks/restore_dtr_images.yml</codeph> is used to restore DTR images</li>
<li><codeph>playbooks/restore_dtr_metadata.yml</codeph> is used to restore DTR metadata</li>
<li><codeph>playbooks/restore_ucp.yml</codeph> is used to restore UCP</li>
</ul>
    
    
</section>    
    

</body>
</topic>

<topic id="convenience-playbooks">
<title>Convenience playbooks</title>
    
<body>
 
<section>

<ul>
<li><codeph>playbooks/clean_all.yml</codeph> powers off and deletes all VMs in your inventory.</li>    
<li><codeph>playbooks/distribute_keys.yml</codeph> distributes public keys between all nodes, to allow each node to 
password-less log in to every other node. As this is not essential and can be regarded as a security risk 
(a worker node probably should not be able to log in to a UCP node, for instance), this playbook is commented out in
 <codeph>site.yml</codeph> by default.</li>    
</ul>
    
</section> 
</body>        
</topic>

<topic id="convenience-scripts">
<title>Convenience scripts</title>
    
<body>
 
<section>

<ul>
<li><codeph>backup.sh</codeph> can be used to take a backup of the swarm, UCP, DTR metadata and the DTR images in one go.</li>
<li><codeph>restore_dtr.sh</codeph> can be used to restore DTR metadata and DTR images.</li>
<li><codeph>scale_worker.sh</codeph> can be used to scale the worker nodes.</li>
</ul>

 
</section>
</body>
</topic>
    
<!--	
        
monitoring_win.yml

internal:
collect_facts.yml
vsphere_plugin_win_ps.yml


???
config_monitoring_files.yml
config_simplivity_backups.yml


-->
</topic>
